import google.generativeai as genai
from google.api_core import exceptions # <-- IMPORTANTE: para capturar o erro espec√≠fico
import time # <-- IMPORTANTE: para a l√≥gica de espera
from app.core.config import settings
import logging
import json
import re # <-- IMPORTANTE: para extrair o tempo de espera do erro
from datetime import datetime
from typing import Optional, List
from app.db import models

logger = logging.getLogger(__name__)

class GeminiService:
    def __init__(self):
        try:
            genai.configure(api_key=settings.GOOGLE_API_KEY)

            self.generation_config = {
                "temperature": 0.75,
                "top_p": 1,
                "top_k": 1,
            }

            # Corrigido para o nome de modelo correto gemini-2.5-flash
            self.model = genai.GenerativeModel(
                model_name='gemini-2.5-flash', # Aten√ß√£o: O modelo 'gemini-2.5-flash' n√£o existe, o correto √© 'gemini-2.5-flash'
                generation_config=self.generation_config
            )
            
            logger.info("‚úÖ Cliente Gemini inicializado com sucesso (gemini-2.5-flash).")
        except Exception as e:
            logger.error(f"üö® ERRO CR√çTICO ao configurar o Gemini: {e}")
            raise

    # --- NOVA L√ìGICA DE RETENTATIVA ---
    def _generate_with_retry(self, model: genai.GenerativeModel, prompt: any, max_retries: int = 3) -> genai.types.GenerateContentResponse:
        """
        Executa a chamada para a API Gemini com uma l√≥gica de retentativa para erros de quota (429).
        """
        attempt = 0
        while attempt < max_retries:
            try:
                # Tenta gerar o conte√∫do
                return model.generate_content(prompt)
            except exceptions.ResourceExhausted as e:
                attempt += 1
                logger.warning(f"Quota da API excedida (429). Tentativa {attempt}/{max_retries}.")
                
                # Extrai o tempo de espera sugerido pela API a partir da mensagem de erro
                # O padr√£o busca por "retry_delay { seconds: XX }"
                match = re.search(r"retry_delay {\s*seconds: (\d+)\s*}", str(e))
                
                if match:
                    wait_time = int(match.group(1))
                    logger.info(f"API sugeriu aguardar {wait_time} segundos. Aguardando...")
                    # Adiciona uma pequena margem para seguran√ßa
                    time.sleep(wait_time + 2) 
                else:
                    # Se n√£o encontrar a sugest√£o, usa um tempo de espera exponencial
                    wait_time = (2 ** attempt) * 5 
                    logger.warning(f"N√£o foi poss√≠vel extrair o 'retry_delay'. Usando espera exponencial: {wait_time}s.")
                    time.sleep(wait_time)
            
            except Exception as e:
                 # Para outros tipos de erro, n√£o tenta novamente e lan√ßa a exce√ß√£o
                logger.error(f"Erro inesperado ao gerar conte√∫do com Gemini: {e}")
                raise e

        # Se todas as tentativas falharem, lan√ßa uma exce√ß√£o final
        raise Exception(f"N√£o foi poss√≠vel obter uma resposta da API Gemini ap√≥s {max_retries} tentativas.")


    def _replace_variables(self, text: str, contact_data: models.Contact) -> str:
        """Substitui vari√°veis din√¢micas no texto, incluindo as novas observa√ß√µes."""
        now = datetime.now()
        days_in_portuguese = ["Segunda-feira", "Ter√ßa-feira", "Quarta-feira", "Quinta-feira", "Sexta-feira", "S√°bado", "Domingo"]
        first_name = contact_data.nome.split(" ")[0] if contact_data.nome else ""
        
        replacements = {
            "{{nome_contato}}": first_name,
            "{{data_atual}}": now.strftime("%d/%m/%Y"),
            "{{dia_semana}}": days_in_portuguese[now.weekday()],
            "{{observacoes_contato}}": contact_data.observacoes or ""
        }
        
        for var, value in replacements.items():
            text = text.replace(var, value)
        return text

    def _format_db_history_to_string(self, db_history: List[dict]) -> str:
        history_lines = []
        for msg in db_history:
            role_text = "Eu" if msg.get("role") == "assistant" else "Contato"
            content = msg.get("content", "")
            history_lines.append(f"- {role_text}: {content}")
        return "\n".join(history_lines)

    def transcribe_and_analyze_media(self, media_data: dict, db_history: List[dict]) -> str:
        """Transcreve √°udio ou analisa imagem/documento no contexto da conversa."""
        logger.info(f"DEBUG: Iniciando transcri√ß√£o/an√°lise para m√≠dia do tipo {media_data.get('mime_type')}")
        
        prompt_parts = []
        
        if 'audio' in media_data['mime_type']:
            task = "Sua √∫nica tarefa √© transcrever o √°udio a seguir. Retorne apenas o texto transcrito, sem adicionar nenhuma outra palavra ou formata√ß√£o."
            prompt_parts.append(task)
            prompt_parts.append(media_data)
        else:
            history_string = self._format_db_history_to_string(db_history)
            task = f"""
            **Contexto da Conversa Anterior:**
            {history_string}

            **Sua Tarefa:**
            Voc√™ recebeu um arquivo (imagem ou documento) do contato. Analise o conte√∫do do arquivo no contexto da conversa acima.
            Sua resposta deve ser um resumo conciso, direto e √∫til do conte√∫do, como se fosse uma anota√ß√£o.
            Exemplo se for uma planta baixa: "O contato enviou a planta do banheiro, destacando a √°rea do box."
            Exemplo se for um cat√°logo: "O contato enviou um cat√°logo de produtos."
            Retorne APENAS o texto do resumo.
            """
            prompt_parts.append(task)
            prompt_parts.append(media_data)

        try:
            media_model = genai.GenerativeModel('gemini-2.5-flash', generation_config=self.generation_config)
            
            # --- MODIFICADO: Usa a nova fun√ß√£o com retentativa ---
            response = self._generate_with_retry(media_model, prompt_parts)
            
            transcription = response.text.strip()
            logger.info(f"DEBUG: Transcri√ß√£o/An√°lise gerada: '{transcription[:100]}...'")
            return transcription
        except Exception as e:
            logger.error(f"Erro ao transcrever/analisar m√≠dia ap√≥s todas as tentativas: {e}")
            return f"[Erro ao processar m√≠dia: {media_data.get('mime_type')}]"

    def generate_initial_message(self, config: models.Config, contact: models.Contact, history_from_api: Optional[List[dict]] = None) -> dict:
        """Gera a mensagem inicial, considerando um hist√≥rico de texto pr√©-existente."""
        # (O resto da sua l√≥gica para montar o prompt continua igual)
        persona_prompt = self._replace_variables(config.persona, contact)
        message_prompt_template = self._replace_variables(config.prompt, contact)
        
        task_instruction = ""
        if history_from_api:
            history_lines = []
            for msg in history_from_api:
                role_text = "Eu" if msg.get("key", {}).get("fromMe") else "Contato"
                message_content = msg.get("message", {})
                text = message_content.get('conversation') or message_content.get('extendedTextMessage', {}).get('text', '[M√≠dia]')
                history_lines.append(f"- {role_text}: {text}")

            history_text_for_prompt = "\n".join(history_lines)
            task_instruction = f"""
            Existe um hist√≥rico de conversa anterior. Sua tarefa √© reengajar o contato, conectando a conversa anterior com o objetivo da campanha atual: "{message_prompt_template}".
            **Hist√≥rico Anterior:**
            {history_text_for_prompt}
            """
        else:
            task_instruction = f"Sua tarefa √© gerar a PRIMEIRA mensagem de prospec√ß√£o, usando como base: {message_prompt_template}"

        prompt = f"""
        **Sua Persona:**
        {persona_prompt}

        **Sua Tarefa:**
        {task_instruction}
        
        **Regra Principal:** Seja amig√°vel, direto e humano. Evite soar como um rob√¥.

        **Formato OBRIGAT√ìRIO:**
        Responda APENAS com um objeto JSON contendo a chave "mensagem_para_enviar".
        """
        try:
            # --- MODIFICADO: Usa a nova fun√ß√£o com retentativa ---
            response = self._generate_with_retry(self.model, prompt)
            
            clean_response = response.text.strip().replace("```json", "").replace("```", "")
            return json.loads(clean_response)
        except Exception as e:
            logger.error(f"Erro ao gerar mensagem inicial com Gemini ap√≥s todas as tentativas: {e}")
            return {"mensagem_para_enviar": None}

    def generate_reply_message(self, config: models.Config, contact: models.Contact, conversation_history_db: List[dict]) -> dict:
        """Gera uma resposta com base no hist√≥rico do DB (que j√° cont√©m transcri√ß√µes)."""
        # (O resto da sua l√≥gica para montar o prompt continua igual)
        persona_prompt = self._replace_variables(config.persona, contact)
        objective_prompt = self._replace_variables(config.prompt, contact)
        history_string = self._format_db_history_to_string(conversation_history_db)

        system_instruction = f"""
        **Sua Persona:**
        {persona_prompt}

        **Objetivo Principal da Campanha:**
        {objective_prompt}

        **Hist√≥rico da Conversa com '{contact.nome}':**
        {history_string}

        **Sua Tarefa:**
        Voc√™ √© um agente de prospec√ß√£o. Com base no hist√≥rico ACIMA, formule a PR√ìXIMA resposta.
        
        **REGRAS DE OURO PARA A RESPOSTA:**
        1.  **SEJA DIRETO E CONCISO:** V√° direto ao ponto. Evite enrola√ß√£o e frases de preenchimento.
        2.  **SOE HUMANO:** Use uma linguagem natural e casual. N√£o pare√ßa um rob√¥.
        3.  **N√ÉO REPITA INFORMA√á√ïES:** O contato j√° sabe do que se trata, n√£o repita o objetivo da campanha a cada mensagem.
        4.  **AVANCE A CONVERSA:** Sua resposta deve sempre tentar avan√ßar na prospec√ß√£o (responder pergunta, sugerir agendamento, etc.), a menos que o contato pe√ßa para parar.

        **Formato OBRIGAT√ìRIO da Resposta:**
        Responda APENAS com um objeto JSON v√°lido com TR√äS chaves:
        1. "mensagem_para_enviar": A resposta em texto. Se decidir esperar, o valor deve ser null.
        2. "nova_situacao": Um novo status para o contato (ex: "Aguardando Resposta", "Reuni√£o Agendada").
        3. "observacoes": Um resumo da intera√ß√£o para salvar internamente (ex: "Cliente demonstrou interesse em agendar.").
        """
        
        final_prompt = "Com base em todas as instru√ß√µes e no hist√≥rico fornecido, gere a resposta em JSON agora."

        try:
            model_with_system_prompt = genai.GenerativeModel(
                model_name='gemini-2.5-flash',
                system_instruction=system_instruction,
                generation_config=self.generation_config
            )

            # --- MODIFICADO: Usa a nova fun√ß√£o com retentativa ---
            response = self._generate_with_retry(model_with_system_prompt, final_prompt)
            
            text_response = response.text
            start_index = text_response.find('{')
            end_index = text_response.rfind('}')
            if start_index == -1 or end_index == -1:
                raise ValueError(f"N√£o foi poss√≠vel encontrar um objeto JSON na resposta: {text_response}")

            json_string = text_response[start_index : end_index + 1]
            return json.loads(json_string)
            
        except Exception as e:
            logger.error(f"Erro ao gerar resposta com Gemini ap√≥s todas as tentativas: {e}")
            return {"mensagem_para_enviar": None, "nova_situacao": "Erro IA", "observacoes": f"Falha da IA: {e}"}

    def generate_followup_message(self, config: models.Config, contact: models.Contact, history: List[dict]) -> dict:
        """Gera uma mensagem de follow-up para um contato que n√£o respondeu."""
        # (O resto da sua l√≥gica para montar o prompt continua igual)
        persona_prompt = self._replace_variables(config.persona, contact)
        objective_prompt = self._replace_variables(config.prompt, contact)
        history_str = self._format_db_history_to_string(history)
        
        prompt = f"""
        **Sua Persona:**
        {persona_prompt}
        
        **Objetivo Principal da Campanha:**
        {objective_prompt}

        **Hist√≥rico da Conversa com '{contact.nome}':**
        {history_str}

        **Sua Tarefa:**
        A √∫ltima mensagem foi sua e o contato n√£o respondeu.
        Sua tarefa √© gerar uma mensagem de follow-up **curta, direta e amig√°vel** para reengajar a conversa. N√£o seja insistente.
        V√° direto ao ponto, perguntando se ele teve tempo de ver a mensagem anterior ou se ainda tem interesse.

        **Formato OBRIGAT√ìRIO da Resposta:**
        Responda APENAS com um objeto JSON v√°lido com DUAS chaves:
        1. "mensagem_para_enviar": A mensagem de follow-up.
        2. "nova_situacao": O novo status, que deve ser "Aguardando Resposta".
        
        Exemplo:
        {{
            "mensagem_para_enviar": "Oi, {contact.nome}! Tudo bem? S√≥ passando pra saber se voc√™ conseguiu ver minha mensagem anterior. üòâ",
            "nova_situacao": "Aguardando Resposta"
        }}
        """
        try:
            # --- MODIFICADO: Usa a nova fun√ß√£o com retentativa ---
            response = self._generate_with_retry(self.model, prompt)

            clean_response = response.text.strip().replace("```json", "").replace("```", "")
            return json.loads(clean_response)
        except Exception as e:
            logger.error(f"Erro ao gerar follow-up com Gemini ap√≥s todas as tentativas: {e}")
            return {"mensagem_para_enviar": None, "nova_situacao": "Erro IA"}

_gemini_service_instance = None
def get_gemini_service():
    global _gemini_service_instance
    if _gemini_service_instance is None:
        _gemini_service_instance = GeminiService()
    return _gemini_service_instance