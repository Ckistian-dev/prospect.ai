import google.generativeai as genai
from app.core.config import settings
import logging
import json
from datetime import datetime
from typing import Optional, List
from app.db import models

logger = logging.getLogger(__name__)

class GeminiService:
    def __init__(self):
        try:
            genai.configure(api_key=settings.GOOGLE_API_KEY)

            # --- MELHORIA 1: Configura√ß√£o centralizada para controle de criatividade e tamanho ---
            self.generation_config = {
                "temperature": 0.75,         # Aumenta a naturalidade e evita repeti√ß√µes
                "top_p": 1,
                "top_k": 1,
            }

            # --- MELHORIA 2: Corrigido nome do modelo e aplicada a configura√ß√£o padr√£o ---
            # O modelo 'gemini-2.5-flash' n√£o existe, o correto seria 'gemini-2.5-flash'.
            self.model = genai.GenerativeModel(
                model_name='gemini-2.5-flash',
                generation_config=self.generation_config
            )
            
            logger.info("‚úÖ Cliente Gemini inicializado com sucesso (gemini-2.5-flash).")
        except Exception as e:
            logger.error(f"üö® ERRO CR√çTICO ao configurar o Gemini: {e}")
            raise

    def _replace_variables(self, text: str, contact_data: models.Contact) -> str:
        """Substitui vari√°veis din√¢micas no texto, incluindo as novas observa√ß√µes."""
        now = datetime.now()
        days_in_portuguese = ["Segunda-feira", "Ter√ßa-feira", "Quarta-feira", "Quinta-feira", "Sexta-feira", "S√°bado", "Domingo"]
        first_name = contact_data.nome.split(" ")[0] if contact_data.nome else ""
        
        replacements = {
            "{{nome_contato}}": first_name,
            "{{data_atual}}": now.strftime("%d/%m/%Y"),
            "{{dia_semana}}": days_in_portuguese[now.weekday()],
            "{{observacoes_contato}}": contact_data.observacoes or ""
        }
        
        for var, value in replacements.items():
            text = text.replace(var, value)
        return text

    def _format_db_history_to_string(self, db_history: List[dict]) -> str:
        history_lines = []
        for msg in db_history:
            role_text = "Eu" if msg.get("role") == "assistant" else "Contato"
            content = msg.get("content", "")
            history_lines.append(f"- {role_text}: {content}")
        return "\n".join(history_lines)

    def transcribe_and_analyze_media(self, media_data: dict, db_history: List[dict]) -> str:
        """Transcreve √°udio ou analisa imagem/documento no contexto da conversa."""
        logger.info(f"DEBUG: Iniciando transcri√ß√£o/an√°lise para m√≠dia do tipo {media_data.get('mime_type')}")
        
        prompt_parts = []
        
        if 'audio' in media_data['mime_type']:
            task = "Sua √∫nica tarefa √© transcrever o √°udio a seguir. Retorne apenas o texto transcrito, sem adicionar nenhuma outra palavra ou formata√ß√£o."
            prompt_parts.append(task)
            prompt_parts.append(media_data)
        else: # Imagem ou Documento
            history_string = self._format_db_history_to_string(db_history)
            # --- MELHORIA 3: Adicionada instru√ß√£o de concis√£o no prompt de an√°lise ---
            task = f"""
            **Contexto da Conversa Anterior:**
            {history_string}

            **Sua Tarefa:**
            Voc√™ recebeu um arquivo (imagem ou documento) do contato. Analise o conte√∫do do arquivo no contexto da conversa acima.
            Sua resposta deve ser um resumo conciso, direto e √∫til do conte√∫do, como se fosse uma anota√ß√£o.
            Exemplo se for uma planta baixa: "O contato enviou a planta do banheiro, destacando a √°rea do box."
            Exemplo se for um cat√°logo: "O contato enviou um cat√°logo de produtos."
            Retorne APENAS o texto do resumo.
            """
            prompt_parts.append(task)
            prompt_parts.append(media_data)

        try:
            # Aplicando a mesma configura√ß√£o ao modelo de m√≠dia
            media_model = genai.GenerativeModel(
                'gemini-2.5-flash',
                generation_config=self.generation_config
            )
            response = media_model.generate_content(prompt_parts)
            transcription = response.text.strip()
            logger.info(f"DEBUG: Transcri√ß√£o/An√°lise gerada: '{transcription[:100]}...'")
            return transcription
        except Exception as e:
            logger.error(f"Erro ao transcrever/analisar m√≠dia: {e}")
            return f"[Erro ao processar m√≠dia: {media_data.get('mime_type')}]"

    def generate_initial_message(self, config: models.Config, contact: models.Contact, history_from_api: Optional[List[dict]] = None) -> dict:
        """Gera a mensagem inicial, considerando um hist√≥rico de texto pr√©-existente."""
        persona_prompt = self._replace_variables(config.persona, contact)
        message_prompt_template = self._replace_variables(config.prompt, contact)
        
        task_instruction = ""
        if history_from_api:
            history_lines = []
            for msg in history_from_api:
                role_text = "Eu" if msg.get("key", {}).get("fromMe") else "Contato"
                message_content = msg.get("message", {})
                text = message_content.get('conversation') or message_content.get('extendedTextMessage', {}).get('text', '[M√≠dia]')
                history_lines.append(f"- {role_text}: {text}")

            history_text_for_prompt = "\n".join(history_lines)
            task_instruction = f"""
            Existe um hist√≥rico de conversa anterior. Sua tarefa √© reengajar o contato, conectando a conversa anterior com o objetivo da campanha atual: "{message_prompt_template}".
            **Hist√≥rico Anterior:**
            {history_text_for_prompt}
            """
        else:
            task_instruction = f"Sua tarefa √© gerar a PRIMEIRA mensagem de prospec√ß√£o, usando como base: {message_prompt_template}"

        # --- MELHORIA 4: Adicionada instru√ß√£o de comportamento no prompt ---
        prompt = f"""
        **Sua Persona:**
        {persona_prompt}

        **Sua Tarefa:**
        {task_instruction}
        **Regra Principal:** Seja amig√°vel, direto e humano. Evite soar como um rob√¥.

        **Formato OBRIGAT√ìRIO:**
        Responda APENAS com um objeto JSON contendo a chave "mensagem_para_enviar".
        """
        try:
            # O self.model j√° tem a configuration_config definida no __init__
            response = self.model.generate_content(prompt)
            clean_response = response.text.strip().replace("```json", "").replace("```", "")
            return json.loads(clean_response)
        except Exception as e:
            logger.error(f"Erro ao gerar mensagem inicial com Gemini: {e}")
            return {"mensagem_para_enviar": None}

    def generate_reply_message(
        self, 
        config: models.Config, 
        contact: models.Contact, 
        conversation_history_db: List[dict]
    ) -> dict:
        """Gera uma resposta com base no hist√≥rico do DB (que j√° cont√©m transcri√ß√µes)."""
        persona_prompt = self._replace_variables(config.persona, contact)
        objective_prompt = self._replace_variables(config.prompt, contact)
        history_string = self._format_db_history_to_string(conversation_history_db)

        # --- MELHORIA 5: Adicionadas instru√ß√µes expl√≠citas para ser direto e humano ---
        system_instruction = f"""
        **Sua Persona:**
        {persona_prompt}

        **Objetivo Principal da Campanha:**
        {objective_prompt}

        **Hist√≥rico da Conversa com '{contact.nome}':**
        {history_string}

        **Sua Tarefa:**
        Voc√™ √© um agente de prospec√ß√£o. Com base no hist√≥rico ACIMA, formule a PR√ìXIMA resposta.
        
        **REGRAS DE OURO PARA A RESPOSTA:**
        1.  **SEJA DIRETO E CONCISO:** V√° direto ao ponto. Evite enrola√ß√£o e frases de preenchimento.
        2.  **SOE HUMANO:** Use uma linguagem natural e casual. N√£o pare√ßa um rob√¥.
        3.  **N√ÉO REPITA INFORMA√á√ïES:** O contato j√° sabe do que se trata, n√£o repita o objetivo da campanha a cada mensagem.
        4.  **AVANCE A CONVERSA:** Sua resposta deve sempre tentar avan√ßar na prospec√ß√£o (responder pergunta, sugerir agendamento, etc.), a menos que o contato pe√ßa para parar.

        **Formato OBRIGAT√ìRIO da Resposta:**
        Responda APENAS com um objeto JSON v√°lido com TR√äS chaves:
        1. "mensagem_para_enviar": A resposta em texto. Se decidir esperar, o valor deve ser null.
        2. "nova_situacao": Um novo status para o contato (ex: "Aguardando Resposta", "Reuni√£o Agendada").
        3. "observacoes": Um resumo da intera√ß√£o para salvar internamente (ex: "Cliente demonstrou interesse em agendar.").
        """
        
        final_prompt = "Com base em todas as instru√ß√µes e no hist√≥rico fornecido, gere a resposta em JSON agora."

        try:
            # Aplicando a configura√ß√£o tamb√©m neste modelo
            model_with_system_prompt = genai.GenerativeModel(
                model_name='gemini-2.5-flash',
                system_instruction=system_instruction,
                generation_config=self.generation_config
            )
            response = model_with_system_prompt.generate_content(final_prompt)
            
            text_response = response.text
            start_index = text_response.find('{')
            end_index = text_response.rfind('}')
            if start_index == -1 or end_index == -1:
                raise ValueError(f"N√£o foi poss√≠vel encontrar um objeto JSON na resposta: {text_response}")

            json_string = text_response[start_index : end_index + 1]
            return json.loads(json_string)
            
        except Exception as e:
            logger.error(f"Erro ao gerar resposta com Gemini: {e}")
            return {"mensagem_para_enviar": None, "nova_situacao": "Erro IA", "observacoes": f"Falha da IA: {e}"}
        
    def generate_followup_message(self, config: models.Config, contact: models.Contact, history: List[dict]) -> dict:
        """Gera uma mensagem de follow-up para um contato que n√£o respondeu."""
        persona_prompt = self._replace_variables(config.persona, contact)
        objective_prompt = self._replace_variables(config.prompt, contact)
        history_str = self._format_db_history_to_string(history)
        
        # --- MELHORIA 6: Refor√ßando a necessidade de ser direto e breve ---
        prompt = f"""
        **Sua Persona:**
        {persona_prompt}
        
        **Objetivo Principal da Campanha:**
        {objective_prompt}

        **Hist√≥rico da Conversa com '{contact.nome}':**
        {history_str}

        **Sua Tarefa:**
        A √∫ltima mensagem foi sua e o contato n√£o respondeu.
        Sua tarefa √© gerar uma mensagem de follow-up **curta, direta e amig√°vel** para reengajar a conversa. N√£o seja insistente.
        V√° direto ao ponto, perguntando se ele teve tempo de ver a mensagem anterior ou se ainda tem interesse.

        **Formato OBRIGAT√ìRIO da Resposta:**
        Responda APENAS com um objeto JSON v√°lido com DUAS chaves:
        1. "mensagem_para_enviar": A mensagem de follow-up.
        2. "nova_situacao": O novo status, que deve ser "Aguardando Resposta".
        
        Exemplo:
        {{
            "mensagem_para_enviar": "Oi, {contact.nome}! Tudo bem? S√≥ passando pra saber se voc√™ conseguiu ver minha mensagem anterior. üòâ",
            "nova_situacao": "Aguardando Resposta"
        }}
        """
        try:
            # O self.model j√° tem a configuration_config definida no __init__
            response = self.model.generate_content(prompt)
            clean_response = response.text.strip().replace("```json", "").replace("```", "")
            return json.loads(clean_response)
        except Exception as e:
            logger.error(f"Erro ao gerar follow-up com Gemini: {e}")
            return {"mensagem_para_enviar": None, "nova_situacao": "Erro IA"}

_gemini_service_instance = None
def get_gemini_service():
    global _gemini_service_instance
    if _gemini_service_instance is None:
        _gemini_service_instance = GeminiService()
    return _gemini_service_instance