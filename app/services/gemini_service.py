import google.generativeai as genai
from app.core.config import settings
import logging
import json
from datetime import datetime
from typing import Optional, List
from app.db import models

logger = logging.getLogger(__name__)

class GeminiService:
    def __init__(self):
        try:
            genai.configure(api_key=settings.GOOGLE_API_KEY)
            self.model = genai.GenerativeModel('gemini-2.5-flash')
            logger.info("‚úÖ Cliente Gemini inicializado com sucesso (gemini-2.5-flash).")
        except Exception as e:
            logger.error(f"üö® ERRO CR√çTICO ao configurar o Gemini: {e}")
            raise

    def _replace_variables(self, text: str, contact_data: models.Contact) -> str:
        now = datetime.now()
        days_in_portuguese = ["Segunda-feira", "Ter√ßa-feira", "Quarta-feira", "Quinta-feira", "Sexta-feira", "S√°bado", "Domingo"]
        first_name = contact_data.nome.split(" ")[0] if contact_data.nome else ""
        replacements = {
            "{{nome_contato}}": first_name,
            "{{data_atual}}": now.strftime("%d/%m/%Y"),
            "{{dia_semana}}": days_in_portuguese[now.weekday()],
        }
        for var, value in replacements.items():
            text = text.replace(var, value)
        return text

    def _format_db_history_to_string(self, db_history: List[dict]) -> str:
        """Converte o hist√≥rico do DB para uma string leg√≠vel para o prompt."""
        history_lines = []
        for msg in db_history:
            role_text = "Eu" if msg.get("role") == "assistant" else "Contato"
            content = msg.get("content", "")
            history_lines.append(f"- {role_text}: {content}")
        return "\n".join(history_lines)

    def transcribe_and_analyze_media(self, media_data: dict, db_history: List[dict]) -> str:
        """Transcreve √°udio ou analisa imagem/documento no contexto da conversa."""
        logger.info(f"DEBUG: Iniciando transcri√ß√£o/an√°lise para m√≠dia do tipo {media_data.get('mime_type')}")
        
        prompt_parts = []
        
        if 'audio' in media_data['mime_type']:
            task = "Sua √∫nica tarefa √© transcrever o √°udio a seguir. Retorne apenas o texto transcrito, sem adicionar nenhuma outra palavra ou formata√ß√£o."
            prompt_parts.append(task)
            prompt_parts.append(media_data)
        else: # Imagem ou Documento
            history_string = self._format_db_history_to_string(db_history)
            task = f"""
            **Contexto da Conversa Anterior:**
            {history_string}

            **Sua Tarefa:**
            Voc√™ recebeu um arquivo (imagem ou documento) do contato. Analise o conte√∫do do arquivo no contexto da conversa acima.
            Sua resposta deve ser um resumo conciso e √∫til do conte√∫do do arquivo, como se voc√™ estivesse fazendo uma anota√ß√£o no hist√≥rico.
            Exemplo se for uma planta baixa: "O contato enviou a planta do banheiro, destacando a √°rea do box, apresentando informa√ß√£o tal"
            Exemplo se for um cat√°logo: "O contato enviou um cat√°logo de produtos, apresentando informa√ß√£o tal"
            Retorne APENAS o texto do resumo.
            """
            prompt_parts.append(task)
            prompt_parts.append(media_data)

        try:
            media_model = genai.GenerativeModel('gemini-2.5-flash')
            response = media_model.generate_content(prompt_parts)
            transcription = response.text.strip()
            logger.info(f"DEBUG: Transcri√ß√£o/An√°lise gerada: '{transcription[:100]}...'")
            return transcription
        except Exception as e:
            logger.error(f"Erro ao transcrever/analisar m√≠dia: {e}")
            return f"[Erro ao processar m√≠dia: {media_data.get('mime_type')}]"

    def generate_initial_message(self, config: models.Config, contact: models.Contact, history_from_api: Optional[List[dict]] = None) -> dict:
        """Gera a mensagem inicial, considerando um hist√≥rico de texto pr√©-existente."""
        persona_prompt = self._replace_variables(config.persona, contact)
        message_prompt_template = self._replace_variables(config.prompt, contact)
        
        task_instruction = ""
        if history_from_api:
            history_lines = []
            for msg in history_from_api:
                 role_text = "Eu" if msg.get("key", {}).get("fromMe") else "Contato"
                 message_content = msg.get("message", {})
                 text = message_content.get('conversation') or message_content.get('extendedTextMessage', {}).get('text', '[M√≠dia]')
                 history_lines.append(f"- {role_text}: {text}")

            history_text_for_prompt = "\n".join(history_lines)
            task_instruction = f"""
            Existe um hist√≥rico de conversa anterior. Sua tarefa √© reengajar o contato, conectando a conversa anterior com o objetivo da campanha atual: "{message_prompt_template}".
            **Hist√≥rico Anterior:**
            {history_text_for_prompt}
            """
        else:
            task_instruction = f"Sua tarefa √© gerar a PRIMEIRA mensagem de prospec√ß√£o, usando como base: {message_prompt_template}"

        prompt = f"""
        **Sua Persona:**
        {persona_prompt}

        **Sua Tarefa:**
        {task_instruction}

        **Formato OBRIGAT√ìRIO:**
        Responda APENAS com um objeto JSON contendo a chave "mensagem_para_enviar".
        """
        try:
            response = self.model.generate_content(prompt)
            clean_response = response.text.strip().replace("```json", "").replace("```", "")
            return json.loads(clean_response)
        except Exception as e:
            logger.error(f"Erro ao gerar mensagem inicial com Gemini: {e}")
            return {"mensagem_para_enviar": None}

    def generate_reply_message(
        self, 
        config: models.Config, 
        contact: models.Contact, 
        conversation_history_db: List[dict]
    ) -> dict:
        """Gera uma resposta com base no hist√≥rico do DB (que j√° cont√©m transcri√ß√µes)."""
        persona_prompt = self._replace_variables(config.persona, contact)
        objective_prompt = self._replace_variables(config.prompt, contact)
        
        history_string = self._format_db_history_to_string(conversation_history_db)

        system_instruction = f"""
        **Sua Persona:**
        {persona_prompt}

        **Objetivo Principal da Campanha:**
        {objective_prompt}

        **Hist√≥rico da Conversa com '{contact.nome}':**
        {history_string}

        **Sua Tarefa:**
        Voc√™ √© um agente de prospec√ß√£o. Com base no hist√≥rico ACIMA, que j√° inclui transcri√ß√µes de √°udios e an√°lises de arquivos, sua tarefa √© formular a PR√ìXIMA resposta. Decida a melhor a√ß√£o:
        - Se for uma pergunta, responda-a de forma concisa.
        - Se a conversa chegou a um ponto de agendamento, sugira hor√°rios.
        - Se o contato pedir para parar, agrade√ßa e sugira encerrar.
        - Se a √∫ltima mensagem foi sua e o contato apenas confirmou algo (ex: "ok", "sim"), talvez seja melhor esperar.
        - Seu objetivo final √© avan√ßar na prospec√ß√£o.

        **Formato OBRIGAT√ìRIO da Resposta:**
        Responda APENAS com um objeto JSON v√°lido com TR√äS chaves:
        1. "mensagem_para_enviar": A resposta em texto. Se decidir esperar, o valor deve ser null.
        2. "nova_situacao": Um novo status para o contato (ex: "Aguardando Resposta", "Reuni√£o Agendada").
        3. "observacoes": Um resumo da intera√ß√£o para salvar internamente (ex: "Cliente demonstrou interesse em agendar.").
        """
        
        final_prompt = "Com base em todas as instru√ß√µes e no hist√≥rico fornecido, gere a resposta em JSON agora."

        try:
            model_with_system_prompt = genai.GenerativeModel(
                model_name='gemini-2.5-flash',
                system_instruction=system_instruction
            )
            response = model_with_system_prompt.generate_content(final_prompt)
            
            text_response = response.text
            start_index = text_response.find('{')
            end_index = text_response.rfind('}')
            if start_index == -1 or end_index == -1:
                raise ValueError(f"N√£o foi poss√≠vel encontrar um objeto JSON na resposta: {text_response}")

            json_string = text_response[start_index : end_index + 1]
            return json.loads(json_string)
            
        except Exception as e:
            logger.error(f"Erro ao gerar resposta com Gemini: {e}")
            return {"mensagem_para_enviar": None, "nova_situacao": "Erro IA", "observacoes": f"Falha da IA: {e}"}

_gemini_service_instance = None
def get_gemini_service():
    global _gemini_service_instance
    if _gemini_service_instance is None:
        _gemini_service_instance = GeminiService()
    return _gemini_service_instance

