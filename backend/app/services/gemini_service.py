import google.generativeai as genai
from google.api_core import exceptions
from datetime import timezone
import logging
import json
from datetime import datetime
import base64
from typing import Optional, List, Dict, Any
import asyncio
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import settings
from app.db import models
from app.crud import crud_user # Import necess√°rio para a fun√ß√£o de d√©bito

logger = logging.getLogger(__name__)

class GeminiService:
    def __init__(self):
        try:
            self.api_keys = [key.strip() for key in settings.GOOGLE_API_KEYS.split(',') if key.strip()]
            if not self.api_keys:
                raise ValueError("Nenhuma chave de API do Google foi encontrada na vari√°vel GOOGLE_API_KEYS.")
            
            self.current_key_index = 0
            self.generation_config = {"temperature": 0.5, "top_p": 1, "top_k": 1}
            
            logger.info(f"‚úÖ Cliente Gemini inicializado com {len(self.api_keys)} chaves de API.")
        except Exception as e:
            logger.error(f"üö® ERRO CR√çTICO ao configurar o Gemini: {e}")
            raise

    # --- M√âTODO ATUALIZADO PARA RECEBER DB E USER E DEBITAR O TOKEN ---
    async def _generate_with_retry_async(
        self, 
        prompt: Any, 
        db: AsyncSession, 
        user: models.User, 
        force_json: bool = True
    ) -> genai.types.GenerateContentResponse:
        """
        Executa a chamada ass√≠ncrona para a API Gemini, com rota√ß√£o de chaves e d√©bito de token no sucesso.
        """
        for i in range(len(self.api_keys)):
            try:
                key_to_use = self.api_keys[self.current_key_index]
                genai.configure(api_key=key_to_use)
                model = genai.GenerativeModel('gemini-2.5-flash')

                gen_config = self.generation_config
                if force_json:
                    gen_config = {**self.generation_config, "response_mime_type": "application/json"}

                response = await model.generate_content_async(prompt, generation_config=gen_config)
                
                # --- L√ìGICA DE D√âBITO CENTRALIZADA ---
                # Se a chamada foi bem-sucedida, debita 1 token do usu√°rio.
                await crud_user.decrement_user_tokens(db, db_user=user, amount=1)
                
                return response

            except exceptions.ResourceExhausted as e:
                logger.warning(f"Chave de API √≠ndice {self.current_key_index} ({key_to_use[:4]}...) atingiu a quota. Rotacionando...")
                self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
                if i < len(self.api_keys) - 1:
                    await asyncio.sleep(1)

            except Exception as e:
                logger.error(f"Erro inesperado ao gerar conte√∫do com Gemini na chave √≠ndice {self.current_key_index}: {e}")
                raise e

        raise exceptions.ResourceExhausted(f"Todas as {len(self.api_keys)} chaves de API atingiram a quota.")


    def _replace_variables_in_dict(self, config_dict: Dict[str, Any], contact_data: models.Contact) -> Dict[str, Any]:
        # (Este m√©todo n√£o muda)
        config_str = json.dumps(config_dict)
        now = datetime.now()
        days_in_portuguese = ["Segunda-feira", "Ter√ßa-feira", "Quarta-feira", "Quinta-feira", "Sexta-feira", "S√°bado", "Domingo"]
        first_name = contact_data.nome.split(" ")[0] if contact_data.nome else ""
        
        replacements = {
            "{{nome_contato}}": first_name,
            "{{data_atual}}": now.strftime("%d/%m/%Y"),
            "{{dia_semana}}": days_in_portuguese[now.weekday()],
            "{{observacoes_contato}}": contact_data.observacoes or ""
        }
        
        for var, value in replacements.items():
            config_str = config_str.replace(var, value)
        
        return json.loads(config_str)

    def _format_history_for_prompt(self, db_history: List[dict]) -> List[Dict[str, str]]:
        # (Este m√©todo n√£o muda)
        history_for_ia = []
        for msg in db_history:
            role = "ia" if msg.get("role") == "assistant" else "contato"
            content = msg.get("content", "")
            history_for_ia.append({"remetente": role, "mensagem": content})
        return history_for_ia

    # --- ASSINATURA ATUALIZADA PARA PASSAR DB E USER ---
    async def transcribe_and_analyze_media(
        self, 
        media_data: dict, 
        config: models.Config,
        db: AsyncSession,
        user: models.User,
        db_history: Optional[List[dict]] = None
    ) -> str:
        logger.info(f"Iniciando transcri√ß√£o/an√°lise para m√≠dia do tipo {media_data.get('mime_type')}")
        prompt_parts = []

        # --- CORRE√á√ÉO IN√çCIO: Convers√£o de m√≠dia para bin√°rio ---
        raw_data = media_data.get("data")
        mime_type = media_data.get("mime_type")

        # Garante que os dados de m√≠dia sejam bin√°rios (bytes) para a API do Gemini.
        # Se recebermos uma string, assumimos que √© base64 e a decodificamos.
        if isinstance(raw_data, str):
            try:
                # Remove o cabe√ßalho de Data URL se presente
                if "base64," in raw_data:
                    raw_data = raw_data.split("base64,")[1]
                
                # Decodifica a string base64 para bytes
                raw_data = base64.b64decode(raw_data)
            except (IndexError, base64.binascii.Error) as e:
                logger.error(f"Falha ao decodificar m√≠dia em base64: {e}")
                return f"[Erro: Formato de m√≠dia inv√°lido ({e})]"

        # A API do Gemini espera que o campo 'data' seja bytes.
        if not isinstance(raw_data, bytes):
            logger.error(f"Os dados da m√≠dia n√£o s√£o bin√°rios e n√£o puderam ser convertidos. Tipo: {type(raw_data)}")
            return "[Erro: Dados de m√≠dia em formato inesperado]"

        file_part = {"mime_type": mime_type, "data": raw_data}

        if 'audio' in media_data['mime_type']:
            # --- CORRE√á√ÉO: Simplifica√ß√£o do prompt de transcri√ß√£o ---
            # O modelo √© mais eficaz para transcri√ß√£o quando o prompt √© direto.
            # Pedir JSON para uma tarefa simples como essa pode confundir o modelo.
            transcription_prompt = "Transcreva este √°udio de forma literal. Retorne apenas o texto transcrito, sem nenhuma palavra ou formata√ß√£o adicional."
            prompt_parts.append(transcription_prompt)
            prompt_parts.append(file_part)
            
            max_retries = 3
            last_error = "Nenhum erro registrado."

            for attempt in range(max_retries):
                try:
                    # force_json=False, pois n√£o esperamos mais um JSON como resposta.
                    response = await self._generate_with_retry_async(prompt_parts, db, user, force_json=False)
                    
                    transcription = response.text.strip()
                    if not transcription:
                        reason = response.candidates[0].finish_reason.name if response.candidates else "UNKNOWN"
                        logger.warning(f"Tentativa {attempt + 1}/{max_retries}: Transcri√ß√£o vazia. Raz√£o: {reason}. Tentando novamente...")
                        last_error = f"Resposta vazia da IA (Raz√£o: {reason})"
                        await asyncio.sleep(1)
                        continue

                    logger.info(f"Transcri√ß√£o de √°udio gerada: '{transcription[:100]}...'")
                    return transcription

                except Exception as e:
                    logger.error(f"Tentativa {attempt + 1}/{max_retries}: Erro ao transcrever √°udio: {e}", exc_info=True)
                    last_error = str(e)
                    await asyncio.sleep(1)

            logger.error(f"Falha ao transcrever √°udio ap√≥s {max_retries} tentativas. √öltimo erro: {last_error}")
            return f"[Erro ao processar √°udio ap√≥s {max_retries} tentativas]"

        else:
            # --- AN√ÅLISE DE IMAGEM/PDF ---
            if db_history is None:
                db_history = []

            formatted_history = self._format_history_for_prompt(db_history or [])
            
            contexto_planilha_str = json.dumps(config.contexto_sheets or {"aviso": "Nenhum contexto de planilha foi fornecido."}, ensure_ascii=False, indent=2)
            historico_conversa_str = json.dumps(formatted_history, ensure_ascii=False, indent=2)

            analysis_prompt_text = f"""
                **Instru√ß√£o Geral:**
                Voc√™ √© um especialista em extra√ß√£o de dados de documentos e imagens. Sua tarefa √© analisar o arquivo enviado (imagem ou documento) e extrair as informa√ß√µes relevantes. O resultado ser√° usado como contexto para outra IA e n√£o deve ter o tom da persona.

                **Regras:**
                1.  **Foco na Extra√ß√£o de Dados:** Sua prioridade √© EXTRAIR os dados importantes do arquivo. Use o hist√≥rico da conversa e o contexto da planilha para entender o que √© relevante.
                2.  **Seja um Extrator, N√£o um Assistente:** Sua resposta deve ser puramente a informa√ß√£o extra√≠da. N√£o converse, n√£o cumprimente, n√£o use a persona do assistente.
                3.  **Resposta Limpa:** Sua resposta final deve ser APENAS o objeto JSON, sem nenhuma outra palavra, t√≠tulo ou formata√ß√£o como ```json.

                **Formato de Resposta Obrigat√≥rio (JSON):**
                Sua resposta DEVE ser um √∫nico objeto JSON v√°lido com a seguinte chave:
                {{
                "analise": "O texto puro da an√°lise/extra√ß√£o do arquivo, seguindo as regras acima."
                }}

                **Contexto para An√°lise:**
                - **Contexto da Planilha:** {contexto_planilha_str}
                - **Hist√≥rico da Conversa:** {historico_conversa_str}

                **Tarefa Imediata:**
                Analise o arquivo a seguir e retorne a extra√ß√£o de dados no formato JSON especificado.
                """
            prompt_parts.append(analysis_prompt_text)
            prompt_parts.append(file_part)

            try:
                response = await self._generate_with_retry_async(prompt_parts, db, user, force_json=True)
                response_json = json.loads(response.text)
                analysis = response_json.get("analise", "[N√£o foi poss√≠vel extrair a an√°lise]").strip()
                logger.info(f"An√°lise de m√≠dia gerada: '{analysis[:100]}...'")
                return analysis
            except Exception as e:
                logger.error(f"Erro ao analisar m√≠dia com prompt JSON: {e}")
                return f"[Erro ao processar m√≠dia: {media_data.get('mime_type')}]"

    # --- ASSINATURA ATUALIZADA PARA PASSAR DB E USER ---
    async def generate_conversation_action(
        self,
        config: models.Config,
        contact: models.Contact,
        conversation_history_db: List[dict],
        mode: str,
        db: AsyncSession,
        user: models.User
    ) -> dict:
        # Substitui a l√≥gica de prompt_config pela nova l√≥gica de contexto
        # A fun√ß√£o _replace_variables_in_dict ainda pode ser √∫til no contexto_sheets se voc√™ usar vari√°veis l√°
        contexto_sheets_processado = self._replace_variables_in_dict(config.contexto_sheets or {}, contact)
        arquivos_drive_processado = config.arquivos_drive or {}

        task_map = {
            'initial': "Gerar a primeira mensagem de prospec√ß√£o para iniciar a conversa. Seja breve e direto.",
            'reply': "Analisar a √∫ltima mensagem do contato e formular a PR√ìXIMA resposta para avan√ßar na conversa, usando o contexto dispon√≠vel.",
            'followup': "Analisar as mensagens e decidir entre continuar o fluxo, fazer um follow-up ou se n√£o √© necess√°rio mais nada apenas retorne 'null' no campo 'mensagem_para_enviar"
        }
        formatted_history = self._format_history_for_prompt(conversation_history_db)
        master_prompt = {
            "instrucao_geral": "Voc√™ √© um assistente de prospec√ß√£o e vendas...",
            "formato_resposta_obrigatorio": {
                "descricao": "Sua resposta DEVE ser um √∫nico objeto JSON v√°lido...",
                "chaves": {
                    "mensagem_para_enviar": "...",
                    "nova_situacao": "Um dos seguintes: 'Aguardando Resposta', 'Lead Qualificado', 'N√£o Interessado', 'Conclu√≠do'.",
                    "observacoes": "Um resumo objetivo da intera√ß√£o para registro interno."
                },
                "regra_importante_variaveis": "CR√çTICO: NUNCA inclua placeholders como {{nome_contato}} na resposta final."
            },
            "contexto_conhecimento": {
                "descricao": "Fonte de verdade prim√°ria para responder perguntas. Use estes dados antes do seu conhecimento geral.",
                "planilhas": contexto_sheets_processado or {"aviso": "Nenhum dado de planilha dispon√≠vel."},
            },
            "arquivos_disponiveis": {
                "descricao": "Estrutura de arquivos que voc√™ pode sugerir ao cliente. Se for enviar um, retorne o 'id' do arquivo no campo 'arquivos_anexos' da resposta.",
                "estrutura": arquivos_drive_processado or {"aviso": "Nenhum arquivo do Drive dispon√≠vel."}
            },
            "dados_atuais_conversa": {
                "contato_nome": contact.nome,
                "contato_numero": contact.whatsapp,
                "contato_observacoes": contact.observacoes,
                "tarefa_imediata": task_map.get(mode, "Continuar a conversa de forma coerente."),
                "historico_conversa": formatted_history
            }
        }

        final_prompt_str = json.dumps(master_prompt, ensure_ascii=False, indent=2)
        max_retries = 3
        last_error = None

        for attempt in range(max_retries):
            try:
                response = await self._generate_with_retry_async(final_prompt_str, db, user, force_json=True)
                clean_response_text = response.text.strip().replace("```json", "").replace("```", "")
                response_data = json.loads(clean_response_text)

                # Valida√ß√£o da resposta: verifica se a mensagem n√£o √© nula ou vazia (ap√≥s remover espa√ßos)
                # A exce√ß√£o √© o modo 'followup', onde uma mensagem vazia pode ser intencional.
                message_to_send = response_data.get("mensagem_para_enviar")
                if mode != 'followup' and (message_to_send is None or not str(message_to_send).strip()):
                    logger.warning(f"Tentativa {attempt + 1}/{max_retries}: IA gerou mensagem vazia. Tentando novamente...")
                    last_error = "IA gerou mensagem vazia."
                    await asyncio.sleep(1)  # Pequena pausa antes de tentar novamente
                    continue

                return response_data  # Retorna a resposta v√°lida

            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(f"Tentativa {attempt + 1}/{max_retries}: Erro de formato na resposta da IA ({e}). Tentando novamente...")
                last_error = f"Erro de formato JSON: {e}"
                await asyncio.sleep(1)
                continue
            except Exception as e:
                logger.error(f"Erro na tentativa {attempt + 1}/{max_retries} ao gerar a√ß√£o (Modo: {mode}): {e}", exc_info=True)
                last_error = str(e)
                break # Sai do loop em caso de erro cr√≠tico (ex: quota)

        # Se todas as tentativas falharem
        logger.error(f"Falha ao gerar a√ß√£o de conversa√ß√£o para o modo '{mode}' ap√≥s {max_retries} tentativas.")
        return {
            "mensagem_para_enviar": None,
            "nova_situacao": "Erro IA",
            "observacoes": f"Falha da IA ap√≥s {max_retries} tentativas: {last_error}"
        }

    async def analyze_prospecting_data(
        self,
        db: AsyncSession,
        user: models.User,
        question: str,
        start_date: Optional[datetime],
        end_date: Optional[datetime]
    ) -> Dict[str, Any]:
        """Usa a IA para analisar dados de prospec√ß√£o com base em uma pergunta do usu√°rio."""
        from app.crud import crud_prospect, crud_config

        logger.info(f"Iniciando an√°lise de dados de prospec√ß√£o para user_id={user.id} com a pergunta: '{question[:100]}...'")

        # Coletar dados relevantes
        prospects = await crud_prospect.get_prospects_by_user(db, user_id=user.id)
        simplified_prospects = []
        for p in prospects:
            if p.created_at.replace(tzinfo=timezone.utc) >= start_date and p.created_at.replace(tzinfo=timezone.utc) <= end_date:
                contacts_summary = {
                    "total": len(p.contacts),
                    "concluido": sum(1 for c in p.contacts if c.situacao == 'Conclu√≠do'),
                    "lead_qualificado": sum(1 for c in p.contacts if c.situacao == 'Lead Qualificado'),
                    "aguardando_resposta": sum(1 for c in p.contacts if c.situacao == 'Aguardando Resposta'),
                }
                simplified_prospects.append({
                    "id": p.id, "nome": p.nome_prospeccao, "status": p.status,
                    "created_at": p.created_at.isoformat(), "contacts_summary": contacts_summary
                })

        analysis_prompt = {
            "objetivo": "Voc√™ √© um analista de vendas s√™nior. Analise os dados de prospec√ß√£o fornecidos para responder √† pergunta do usu√°rio. Sua resposta DEVE ser um objeto JSON.",
            "pergunta_usuario": question,
            "dados_contexto": {
                "resumo_usuario": {"id": user.id, "email": user.email},
                "prospeccoes_no_periodo": simplified_prospects
            },
            "formato_resposta_obrigatorio": {
                "descricao": "Sua resposta DEVE ser um √∫nico objeto JSON v√°lido. Siga a estrutura sugerida.",
                "estrutura_sugerida": {
                    "diagnostico_geral": "Um par√°grafo resumindo a situa√ß√£o.",
                    "principais_pontos_de_friccao": [
                        {"area": "Nome da √Årea (ex: Abertura da conversa)", "observacoes": "Detalhes observados.", "impacto_na_conversao": "Alto/M√©dio/Baixo"}
                    ],
                    "insights_acionaveis": [
                        {"titulo": "T√≠tulo da Sugest√£o", "sugestoes": ["Sugest√£o 1.", "Sugest√£o 2."]}
                    ],
                    "proximos_passos_recomendados": "Recomenda√ß√£o final."
                }
            }
        }
        
        response = await self._generate_with_retry_async(json.dumps(analysis_prompt, ensure_ascii=False), db, user, force_json=True)
        return json.loads(response.text)

_gemini_service_instance = None
def get_gemini_service():
    global _gemini_service_instance
    if _gemini_service_instance is None:
        _gemini_service_instance = GeminiService()
    return _gemini_service_instance